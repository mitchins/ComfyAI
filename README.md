# **ComfyAI â€“ LLM-Powered Vision & Text Query Node for ComfyUI**  

ğŸš€ **ComfyAI** is an advanced **LLM-powered query node** for **ComfyUI**, enabling both **text-based and vision-based inference** using multimodal models like **Qwen-VL** and **Llava**.  

This project **isolates CUDA inference in a separate worker process**, ensuring that **ComfyUI remains stable even if CUDA crashes**.  

---

## **âœ¨ Features**  

- âœ… **Text & Vision-Based LLM Inference** â€“ Process **both images and text** in ComfyUI.  
- âœ… **Multimodal Model Support** â€“ Works with **Qwen-VL**, **Llava**, and more.  
- âœ… **Stable & Resilient** â€“ Runs inference in an **isolated worker process** to prevent UI crashes.  
- âœ… **Parallelized Processing** â€“ Uses **multiprocessing** for fast, efficient LLM queries.  
- âœ… **Optimized Image Handling** â€“ Minimizes memory usage with **controlled tokenization**.  

---

## **ğŸ“Œ Supported Models**  

Currently supported models:  
- **Qwen-2.5VL** (`qwen2_5_vl`) â€“ Strong multimodal (text+vision) model.  
- **Llava** (`llava`) â€“ Vision-language AI for image understanding.  

âœ… **Recommended Model:**  
- **Llava-7B (bnb4) from Unsloth** â€“ **Tested & performs exceptionally well!**  
- **Qwen2.5-VL-3B-Instruct (bnb4)** â€“ Good, but **Llava-7B handles instructions significantly better.**  
- Supports **BitsAndBytes 4bit/8bit quantization** for efficiency.  

ğŸš€ **Planned Support:**  
- **mLLaMA & Pixtral** â€“ Requires additional integration (not yet implemented).  

---

## **ğŸ“¥ Installation**  

### **ğŸ”§ Prerequisites**  
Ensure you have the following installed:  
- **Python 3.10+**  
- **PyTorch with CUDA** (`torch + torchvision`)  
- **Hugging Face Transformers** (`transformers`)  
- **ComfyUI** (installed separately)  

### **ğŸ“Œ Install ComfyAI (from your ComfyUI installation folder)**  

```bash
cd custom_nodes
git clone https://github.com/mitchins/ComfyAI.git
cd ComfyAI
pip install -r requirements.txt
```

---

## **ğŸš€ Usage**  

### **ğŸ“Œ Using the Query Node in ComfyUI**  

1. **Start ComfyUI** (ensure itâ€™s installed and running).  
2. **Load the custom node from ComfyAI**.  
3. **Connect image/text inputs** and send queries.  
4. **The worker process handles inference asynchronously**.  

---

### **ğŸ“Œ Use Case 1 - Single Image â†’ Text Output**  

To **describe an image**, pass it as `sample`. The `reference` input is only used for comparisons.  

**Example Workflow:**  
![Single Image Example](Example01.png)  

ğŸ“ **Example Prompt:**  
> *"You are an interface for stable diffusion. Provide a prompt to generate an image like this one."*  

---

### **ğŸ“Œ Use Case 2 - Comparing Two Images (Boolean Output)**  

The **Vision LLM** can compare **two images** and **output a True/False result**.  

**Example Workflow:**  
![Image Comparison Example](Example02.png)  

ğŸ“ **Example Prompt:**  
> *"Answer yes or no, are the following two images similarly themed?"*  

ğŸ’¡ **Tip:** This library includes a **`ConditionalSave` node**, which allows saving an image **only if a boolean condition is met**.  

---

### **ğŸ“Œ Use Case 3 - AI-Generated Prompt from an Image**  

The **Vision LLM** can generate text prompts **based on an input image**, making it useful for **Stable Diffusion automation**.  

**Example Workflow:**  
![AI Generating Prompts](Example03.png)  

ğŸ“ **Example Prompt:**  
> *"Describe this image as a Stable Diffusion prompt."*  

**ComfyAI automatically writes a prompt**, which is then used to generate a similar image!  

---

### **ğŸ“Œ Use Case 4 - Combined Image Comparison + AI-Generated Prompt**  

This setup **first compares two images for similarity**, then **generates a Stable Diffusion prompt to recreate it**.  

**Example Workflow:**  
![AI Prompting AI](Example04.png)  

ğŸ“ **Example Prompt:**  
> *"Given the image provided, output the prompt for a Stable Diffusion image service to create one exactly like it. Ensure the style is the same. Be direct but ensure details are well-defined."*  

ğŸ’¡ **This is useful for**:  
- **Style transfer**  
- **Recreating an image in a different medium**  
- **Refining AI-generated art iteratively**  

---

## **ğŸ› ï¸ Configuration**  

### **ğŸ” Changing the Model**  
To use a different model, **select it inside the node in your ComfyUI workflow**.  

ğŸ’¡ **Example:**  
If you want to use a **Llava-7B model**, make sure itâ€™s downloaded:  

```bash
huggingface-cli download unsloth/llava-1.5-7b-hf-bnb-4bit --all
```

Then, **select it inside the ComfyUI node settings**.  

---

### **ğŸ“œ Logging**  
Logs are saved to `worker.log` in the package directory.  

ğŸ“Œ **Monitor logs in real-time:**  
```bash
tail -f custom_nodes/ComfyNodes/transformer_worker/worker.log
```

---

## **ğŸ“… Roadmap**  

ğŸš€ **Planned improvements:**  
- âœ… **Expanding model support** (mLLaMA, Pixtral, ONNX models like Phi-3.5 Vision).  
- âœ… **Adding API-based inference** (Ollama, OpenAI endpoints).  
- âœ… **Performance optimizations** to further reduce memory usage.  

---

## **ğŸ“œ License**  

This project is licensed under the **MIT License**. See `LICENSE` for details.  

---

## **ğŸš€ Stay Updated**  

â­ **Star this repo** if you find it useful!  
ğŸ“£ **Issues, feedback, and contributions are welcome.**  

Happy coding! ğŸ¨ğŸ¤–  

